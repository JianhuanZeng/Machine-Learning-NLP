{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tools # self-defined functions\n",
    "import warnings,sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76r64</th>\n",
       "      <td>Crito</td>\n",
       "      <td>1223866465</td>\n",
       "      <td>76r64</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>ptsd</td>\n",
       "      <td>r/ptsd</td>\n",
       "      <td>public</td>\n",
       "      <td>A plea for the wider victims of traumatic stress.</td>\n",
       "      <td>http://www.jpost.com/servlet/Satellite?cid=122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7goht</th>\n",
       "      <td>Crito</td>\n",
       "      <td>1228150531</td>\n",
       "      <td>7goht</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>ptsd</td>\n",
       "      <td>r/ptsd</td>\n",
       "      <td>public</td>\n",
       "      <td>Cortisone may help ease PTSD</td>\n",
       "      <td>http://www.jpost.com/servlet/Satellite?cid=122...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7guki</th>\n",
       "      <td>socialcelebs</td>\n",
       "      <td>1228219003</td>\n",
       "      <td>7guki</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>mentalhealth</td>\n",
       "      <td>r/mentalhealth</td>\n",
       "      <td>public</td>\n",
       "      <td>Eliminate Anxiety Attacks for Good</td>\n",
       "      <td>http://www.relief-anxiety.info/eliminate-anxie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7gxll</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1228244460</td>\n",
       "      <td>7gxll</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>EatingDisorders</td>\n",
       "      <td>r/EatingDisorders</td>\n",
       "      <td>restricted</td>\n",
       "      <td>Bulimia just gets on my nerves!</td>\n",
       "      <td>http://operationlola.wordpress.com/2008/11/30/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7gxm3</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1228244538</td>\n",
       "      <td>7gxm3</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>EatingDisorders</td>\n",
       "      <td>r/EatingDisorders</td>\n",
       "      <td>restricted</td>\n",
       "      <td>Why throwing up makes you GAIN weight</td>\n",
       "      <td>http://operationlola.wordpress.com/2008/11/28/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author  created_utc     id  num_comments selftext  \\\n",
       "id                                                               \n",
       "76r64         Crito   1223866465  76r64             0            \n",
       "7goht         Crito   1228150531  7goht             0            \n",
       "7guki  socialcelebs   1228219003  7guki             0            \n",
       "7gxll     [deleted]   1228244460  7gxll             1            \n",
       "7gxm3     [deleted]   1228244538  7gxm3             1            \n",
       "\n",
       "             subreddit subreddit_name_prefixed subreddit_type  \\\n",
       "id                                                              \n",
       "76r64             ptsd                  r/ptsd         public   \n",
       "7goht             ptsd                  r/ptsd         public   \n",
       "7guki     mentalhealth          r/mentalhealth         public   \n",
       "7gxll  EatingDisorders       r/EatingDisorders     restricted   \n",
       "7gxm3  EatingDisorders       r/EatingDisorders     restricted   \n",
       "\n",
       "                                                   title  \\\n",
       "id                                                         \n",
       "76r64  A plea for the wider victims of traumatic stress.   \n",
       "7goht                       Cortisone may help ease PTSD   \n",
       "7guki                 Eliminate Anxiety Attacks for Good   \n",
       "7gxll                    Bulimia just gets on my nerves!   \n",
       "7gxm3              Why throwing up makes you GAIN weight   \n",
       "\n",
       "                                                     url  \n",
       "id                                                        \n",
       "76r64  http://www.jpost.com/servlet/Satellite?cid=122...  \n",
       "7goht  http://www.jpost.com/servlet/Satellite?cid=122...  \n",
       "7guki  http://www.relief-anxiety.info/eliminate-anxie...  \n",
       "7gxll  http://operationlola.wordpress.com/2008/11/30/...  \n",
       "7gxm3  http://operationlola.wordpress.com/2008/11/28/...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "load_file = '../datasets/reddit_submissions.json'\n",
    "someposts = pd.read_json(load_file , lines=True)\n",
    "someposts.index = someposts['id']\n",
    "someposts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### record_process\n",
    "Preprocesses the raw data, with the following guidelines:\n",
    "  - Exclude rows from the training set where `selftext` is a blank string, or\n",
    "    has the values of either `\"[removed\"]` or `\"[deleted]\"`.\n",
    "  - Exclude rows with less than `5` comments.\n",
    "  - Only use the `title` and `selftext` fields as a source of features.\n",
    "  - Make a decision on how to handle subreddit categories with fewer than\n",
    "    1000 examples that simply merge them into /**one category**/, because the sum amount of rare categories is about 1142, a small amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 236742 records after processing\n",
      "The sum of rare categories is 1142\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96l9t</th>\n",
       "      <td>Coping with panic/anxiety attacks. You tips?</td>\n",
       "      <td>Following on from the Onion article, and some ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96zvm</th>\n",
       "      <td>Nothing much to keep me going (reintroduced)</td>\n",
       "      <td>About 4 months ago I posted something with bas...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972xv</th>\n",
       "      <td>Would it be a good idea to pool our resources ...</td>\n",
       "      <td>Hi all,\\n\\nI had an idea last night and I was ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976on</th>\n",
       "      <td>This is my declaration of Interdependence</td>\n",
       "      <td>I am a fraud. I have spent a lifetime distanci...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977ls</th>\n",
       "      <td>I'm considering submitting myself to a psychia...</td>\n",
       "      <td>I won't go too into the details. Suffice to sa...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "id                                                         \n",
       "96l9t       Coping with panic/anxiety attacks. You tips?   \n",
       "96zvm       Nothing much to keep me going (reintroduced)   \n",
       "972xv  Would it be a good idea to pool our resources ...   \n",
       "976on          This is my declaration of Interdependence   \n",
       "977ls  I'm considering submitting myself to a psychia...   \n",
       "\n",
       "                                                selftext  target  \n",
       "id                                                                \n",
       "96l9t  Following on from the Onion article, and some ...       6  \n",
       "96zvm  About 4 months ago I posted something with bas...       6  \n",
       "972xv  Hi all,\\n\\nI had an idea last night and I was ...       6  \n",
       "976on  I am a fraud. I have spent a lifetime distanci...       6  \n",
       "977ls  I won't go too into the details. Suffice to sa...       8  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_mappings, someposts = tools.record_process(someposts)\n",
    "someposts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Anger',\n",
       " 1: 'BPD',\n",
       " 2: 'BipolarReddit',\n",
       " 3: 'EatingDisorders',\n",
       " 4: 'MMFB',\n",
       " 5: 'StopSelfHarm',\n",
       " 6: 'SuicideWatch',\n",
       " 7: 'alcoholism',\n",
       " 8: 'depression',\n",
       " 9: 'dpdr',\n",
       " 10: 'getting_over_it',\n",
       " 11: 'mentalhealth',\n",
       " 12: 'others',\n",
       " 13: 'ptsd',\n",
       " 14: 'rapecounseling',\n",
       " 15: 'schizophrenia',\n",
       " 16: 'socialanxiety'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('the integer to subreddit:')\n",
    "subreddit_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     91554\n",
       "6     54735\n",
       "1     16850\n",
       "2     14157\n",
       "16    12618\n",
       "15    10242\n",
       "11     7554\n",
       "4      6569\n",
       "7      4931\n",
       "13     4391\n",
       "14     2911\n",
       "9      2654\n",
       "5      2044\n",
       "10     1985\n",
       "3      1218\n",
       "0      1187\n",
       "12     1142\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "someposts['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_process\n",
    "+ I process at the word level. \n",
    "+ I not only remove **punctuation**, but also do **stemming**. The rate of amount of unique word with/without stemming = 0.6, which may effect on results.\n",
    "+ I do not remove stop words in order to capture contextual features, but we can look back in the later iteration to remove the stop words since the classification is topic-based and I want to save time in the training by dealing with less words. \n",
    "+ I combine the title info with the selftext content by adding **End** tokens in text process in the future step \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 82477 unique words tokens.\n",
      "Using vocabulary size 5000.\n",
      "The least frequent word in our vocabulary is 'alzheim' and appeared 181 times.\n",
      "\n",
      "Example sentence: Following on from the Onion article, and some suggestions that a discussion would be good, can anyone share their tips for dealing with this?\n",
      "\n",
      "Example sentence after processing: ['follow', 'on', 'from', 'the', 'UNKNOWN_TOKEN', 'articl', 'and', 'some', 'suggest', 'that', 'a', 'discuss', 'would', 'be', 'good', 'can', 'anyon', 'share', 'their', 'tip', 'for', 'deal', 'with', 'thi']\n",
      "\n",
      "Example input sentence: [618, 28, 70, 3, 4999, 1564, 2, 87, 665, 9, 4, 928, 69, 20, 115, 29, 116, 451, 196, 1048, 15, 267, 19, 18]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 5000\n",
    "index_to_word, data  = tools.textprocess(someposts[['title','selftext']].apply(lambda x: 'END. '.join(x), axis=1).values, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('ModelTraining/index_to_word.csv','w') as f:\n",
    "    f.write(str(index_to_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partition_dataset\n",
    "Partitions the model-ready data into train, validation, and test sets. Since we have 240k records(a relative large set), I picks train/validation/test ratio **80%, 10%, 10%**.\n",
    "My training process will not use test set for unleaky info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 236742 records after processing\n",
      "There are 189393,23675,23674 records for train, validation, and test sets\n"
     ]
    }
   ],
   "source": [
    "# partitions the model-ready data into train, validation, and test sets.\n",
    "print('There are {} records after processing'.format(len(someposts)))\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = tools.partition_dataset(someposts, data)\n",
    "print('There are {},{},{} records for train, validation, and test sets'.format(len(X_train), len(X_val), len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Possible models:\n",
    "+ Bag of Words/Bigrams + LR/SVM\n",
    "+ Average Embedding + LR\n",
    "+ LDA\n",
    "+ Tree Kernels\n",
    "+ @RNN: I try **word-based 1-layer LSTM** as baseline \n",
    "+ @CNN: I secondly try **character-level CNN**\n",
    "+ @RCNN: I finally will try the advanced RCNN\n",
    "\n",
    "I first try the simplest word-based LSTM, then try character-level CNN and finally try advanced word-based RCNN if possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: word-based one-layer LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189393, 300)\n"
     ]
    }
   ],
   "source": [
    "# Cut texts after this number of words \n",
    "max_len = 300\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=max_len)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 16)           80000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               74240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 156,433\n",
      "Trainable params: 156,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# embedding and train\n",
    "embedding_dimension = 16\n",
    "n_classes = len(subreddit_mappings)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dimension, input_length=max_len))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# complie\n",
    "model.compile('adam', 'sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 189393 samples, validate on 23675 samples\n",
      "Epoch 1/5\n",
      "189393/189393 [==============================] - 2503s 13ms/step - loss: 1.6889 - acc: 0.4617 - val_loss: 1.3811 - val_acc: 0.5653\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.38109, saving model to lstm_1st.hdf5\n",
      "Epoch 2/5\n",
      "189393/189393 [==============================] - 2413s 13ms/step - loss: 1.3874 - acc: 0.5602 - val_loss: 1.2269 - val_acc: 0.6086\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.38109 to 1.22686, saving model to lstm_1st.hdf5\n",
      "Epoch 3/5\n",
      "189393/189393 [==============================] - 3350s 18ms/step - loss: 1.2123 - acc: 0.6096 - val_loss: 1.1656 - val_acc: 0.6239\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.22686 to 1.16564, saving model to lstm_1st.hdf5\n",
      "Epoch 4/5\n",
      "189393/189393 [==============================] - 3233s 17ms/step - loss: 1.1484 - acc: 0.6259 - val_loss: 1.1239 - val_acc: 0.6324\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.16564 to 1.12395, saving model to lstm_1st.hdf5\n",
      "Epoch 5/5\n",
      "189393/189393 [==============================] - 2517s 13ms/step - loss: 1.1076 - acc: 0.6361 - val_loss: 1.1040 - val_acc: 0.6381\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.12395 to 1.10398, saving model to lstm_1st.hdf5\n"
     ]
    }
   ],
   "source": [
    "# prepare for training\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='acc', \n",
    "                                                  min_delta=0.0001,\n",
    "                                                  patience=1, \n",
    "                                                  verbose=1)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('ModelTraining/lstm_1st.hdf5', \n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True) \n",
    "# training\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size = 32,\n",
    "                    epochs=5, \n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint, early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and save\n",
    "Using **f1-score** to capture precision and recall, this model is good to classify 3-'EatingDisorders' with 0.80 and 7-'alcoholism' with 0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.31      0.40       127\n",
      "          1       0.72      0.56      0.63      1731\n",
      "          2       0.74      0.61      0.67      1423\n",
      "          3       0.83      0.77      0.80       123\n",
      "          4       0.80      0.19      0.30       657\n",
      "          5       0.57      0.42      0.49       193\n",
      "          6       0.61      0.68      0.64      5483\n",
      "          7       0.82      0.79      0.81       502\n",
      "          8       0.61      0.74      0.67      9109\n",
      "          9       0.86      0.57      0.68       272\n",
      "         10       0.00      0.00      0.00       205\n",
      "         11       0.31      0.04      0.07       737\n",
      "         12       0.00      0.00      0.00       114\n",
      "         13       0.76      0.58      0.66       476\n",
      "         14       0.65      0.64      0.65       276\n",
      "         15       0.61      0.62      0.61      1001\n",
      "         16       0.71      0.62      0.66      1245\n",
      "\n",
      "avg / total       0.63      0.64      0.62     23674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict and evaluate\n",
    "results = model.predict(X_test)\n",
    "predictions = results.argmax(axis = 1)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save partly trained model\n",
    "model.save('ModelTraining/partly_trained_lstm_0613.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
